<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-10-18T14:19:47-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Interactive Audio Lab</title><subtitle>Lab description</subtitle><entry><title type="html">TorchCrepe pitch tracker passes 2,000,0000 downloads.</title><link href="http://localhost:4000/news/2023/10/18/TorchCrepe.html" rel="alternate" type="text/html" title="TorchCrepe pitch tracker passes 2,000,0000 downloads." /><published>2023-10-18T11:00:00-04:00</published><updated>2023-10-18T11:00:00-04:00</updated><id>http://localhost:4000/news/2023/10/18/TorchCrepe</id><content type="html" xml:base="http://localhost:4000/news/2023/10/18/TorchCrepe.html"><![CDATA[<p>Lab member Max Morrison’s <a href="https://github.com/maxrmorrison/torchcrepe">TorchCrepe</a> pitch tracker implementation passes 2,000,0000 downloads.
Click <a href="https://github.com/maxrmorrison/torchcrepe"><strong>HERE</strong></a> to try out TorchCrepe for youreslf!</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Lab member Max Morrison’s TorchCrepe pitch tracker implementation passes 2,000,0000 downloads. Click HERE to try out TorchCrepe for youreslf!]]></summary></entry><entry><title type="html">Try the VampNet demo: Music Generation via Masked Acoustic Token Modeling.</title><link href="http://localhost:4000/news/2023/07/01/VampNet.html" rel="alternate" type="text/html" title="Try the VampNet demo: Music Generation via Masked Acoustic Token Modeling." /><published>2023-07-01T11:00:00-04:00</published><updated>2023-07-01T11:00:00-04:00</updated><id>http://localhost:4000/news/2023/07/01/VampNet</id><content type="html" xml:base="http://localhost:4000/news/2023/07/01/VampNet.html"><![CDATA[<p>Our paper <a href="/project/music-audio-generation.html">“VAMPNET: Music Generation via Masked Acoustic Token Modeling”</a> was accepted to ISMIR 2023.</p>

<p>Click <a href="https://huggingface.co/spaces/descript/vampnet"><strong>HERE</strong></a> to try out VampNet!</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Our paper “VAMPNET: Music Generation via Masked Acoustic Token Modeling” was accepted to ISMIR 2023.]]></summary></entry><entry><title type="html">$440K grant from NSF: Engaging Blind and Visually Impaired Youth in Computer Science through Music Programming</title><link href="http://localhost:4000/news/2023/06/01/NSF-grant.html" rel="alternate" type="text/html" title="$440K grant from NSF: Engaging Blind and Visually Impaired Youth in Computer Science through Music Programming" /><published>2023-06-01T11:00:00-04:00</published><updated>2023-06-01T11:00:00-04:00</updated><id>http://localhost:4000/news/2023/06/01/NSF-grant</id><content type="html" xml:base="http://localhost:4000/news/2023/06/01/NSF-grant.html"><![CDATA[<p>Our collaboration with Georgia Tech, entitiled <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2300633&amp;HistoricalAwards=false">“Collaborative Research: Engaging Blind and Visually Impaired Youth in Computer Science through Music Programming”.</a> has been funded by the Discovery Research preK-12 (DRK-12) program, the Innovative Technology Experiences for Students and Teachers (ITEST) program, and the  the CS for All: Research and RPPs program.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Our collaboration with Georgia Tech, entitiled “Collaborative Research: Engaging Blind and Visually Impaired Youth in Computer Science through Music Programming”. has been funded by the Discovery Research preK-12 (DRK-12) program, the Innovative Technology Experiences for Students and Teachers (ITEST) program, and the the CS for All: Research and RPPs program.]]></summary></entry><entry><title type="html">$100K grant from Sony to fund speech generation</title><link href="http://localhost:4000/news/2022/10/01/Sony-grant.html" rel="alternate" type="text/html" title="$100K grant from Sony to fund speech generation" /><published>2022-10-01T11:00:00-04:00</published><updated>2022-10-01T11:00:00-04:00</updated><id>http://localhost:4000/news/2022/10/01/Sony-grant</id><content type="html" xml:base="http://localhost:4000/news/2022/10/01/Sony-grant.html"><![CDATA[<p>Sony corporation has awarded us $100,000 as part of their Research Award Program to fund our research grant titled “Expressive and Controllable Voice Synthesis and Conversion”.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Sony corporation has awarded us $100,000 as part of their Research Award Program to fund our research grant titled “Expressive and Controllable Voice Synthesis and Conversion”.]]></summary></entry><entry><title type="html">$1.8 million Future of Work award from NSF</title><link href="http://localhost:4000/news/2022/09/12/NSF-grant.html" rel="alternate" type="text/html" title="$1.8 million Future of Work award from NSF" /><published>2022-09-12T11:00:00-04:00</published><updated>2022-09-12T11:00:00-04:00</updated><id>http://localhost:4000/news/2022/09/12/NSF-grant</id><content type="html" xml:base="http://localhost:4000/news/2022/09/12/NSF-grant.html"><![CDATA[<p>TEAMuP, Our collaboration with the University of Rochester has been awarded a $1.8 million Future of Work at the Human-Technology Frontier award from NSF. For more details on what we’re doing see our project <a href="/project/audacity.html">Deep Learning Tools for Audacity</a> or see <a href="https://www.mccormick.northwestern.edu/computer-science/news-events/news/articles/2022/connecting-deep-learning-developers-with-sound-artists.html">this story</a>.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[TEAMuP, Our collaboration with the University of Rochester has been awarded a $1.8 million Future of Work at the Human-Technology Frontier award from NSF. For more details on what we’re doing see our project Deep Learning Tools for Audacity or see this story.]]></summary></entry><entry><title type="html">Our tech inside Adobe’s new AI-powered audio editor</title><link href="http://localhost:4000/news/2022/01/01/adobe-project-shasta.html" rel="alternate" type="text/html" title="Our tech inside Adobe’s new AI-powered audio editor" /><published>2022-01-01T10:00:00-05:00</published><updated>2022-01-01T10:00:00-05:00</updated><id>http://localhost:4000/news/2022/01/01/adobe-project-shasta</id><content type="html" xml:base="http://localhost:4000/news/2022/01/01/adobe-project-shasta.html"><![CDATA[<p>Adobe just announced their <a href="https://pages.adobe.com/shasta/">Project Shasta</a> web-based AI-powered audio editing and recording suite. This uses the work in our patent application <a href="https://patents.google.com/patent/US11138989B2/en?inventor=bryan+pardo&amp;oq=bryan+pardo">“Sound quality prediction and interface to facilitate high-quality voice recordings”</a> to ensure users make better recordings.</p>

<p>See the <a href="https://www.youtube.com/watch?v=X4LV-9OPYJ0&amp;t=1s">Voice Assist video</a> or the our 2019 CHI paper <a href="https://interactiveaudiolab.github.io/assets/papers/seetharaman_voiceassist_chi19.pdf">“VoiceAssist: Guiding Users to High-Quality Voice Recordings”</a>for the technical details.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Adobe just announced their Project Shasta web-based AI-powered audio editing and recording suite. This uses the work in our patent application “Sound quality prediction and interface to facilitate high-quality voice recordings” to ensure users make better recordings.]]></summary></entry><entry><title type="html">Bose releases hearing aids using our tech</title><link href="http://localhost:4000/news/2021/12/01/bose-hearing-aids.html" rel="alternate" type="text/html" title="Bose releases hearing aids using our tech" /><published>2021-12-01T10:00:00-05:00</published><updated>2021-12-01T10:00:00-05:00</updated><id>http://localhost:4000/news/2021/12/01/bose-hearing-aids</id><content type="html" xml:base="http://localhost:4000/news/2021/12/01/bose-hearing-aids.html"><![CDATA[<p>Bose has released their new <a href="https://www.bose.com/en_us/products/headphones/earbuds/soundcontrol-hearing-aids.html?mc=25_PS_SN_BO_00_GO_&amp;gclid=CjwKCAiA5t-OBhByEiwAhR-hm_LLxK00tfy5Bx_pIy2oClM1RJzp1ZSO0KiPLCNIlPFQAxMNX9SQ8xoCSSsQAvD_BwE&amp;gclsrc=aw.ds#v=soundcontrol_hearing_aids_gray">SoundControl hearing aids</a>, which use the work in our patent <a href="https://patents.google.com/patent/USRE48462E1/en?inventor=bryan+pardo&amp;oq=bryan+pardo">“Systems, methods, and apparatus for equalization preference learning”</a>.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Bose has released their new SoundControl hearing aids, which use the work in our patent “Systems, methods, and apparatus for equalization preference learning”.]]></summary></entry></feed>