<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-17T08:39:11-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Interactive Audio Lab</title><subtitle>Lab description</subtitle><entry><title type="html">New papers at Interspeech and ISMIR</title><link href="http://localhost:4000/news/2024/09/01/Interspeech.html" rel="alternate" type="text/html" title="New papers at Interspeech and ISMIR" /><published>2024-09-01T11:00:00-04:00</published><updated>2024-09-01T11:00:00-04:00</updated><id>http://localhost:4000/news/2024/09/01/Interspeech</id><content type="html" xml:base="http://localhost:4000/news/2024/09/01/Interspeech.html"><![CDATA[<p>Go to our <a href="/publications/">papers</a> page to see our latest <a href="https://interspeech2024.org">Interspeech</a> and <a href="https://ismir2024.ismir.net">ISMIR</a> publications</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Go to our papers page to see our latest Interspeech and ISMIR publications]]></summary></entry><entry><title type="html">Max Morrison defends dissertation</title><link href="http://localhost:4000/news/2024/06/01/Morrison-graduates.html" rel="alternate" type="text/html" title="Max Morrison defends dissertation" /><published>2024-06-01T11:00:00-04:00</published><updated>2024-06-01T11:00:00-04:00</updated><id>http://localhost:4000/news/2024/06/01/Morrison-graduates</id><content type="html" xml:base="http://localhost:4000/news/2024/06/01/Morrison-graduates.html"><![CDATA[<p>Congratulations to <a href="https://www.maxrmorrison.com">Max Morrison</a> on defending his thesis <a href="https://www.maxrmorrison.com/pdfs/thesis.pdf">Interpretable Speech Representation and Editing
</a>.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Congratulations to Max Morrison on defending his thesis Interpretable Speech Representation and Editing .]]></summary></entry><entry><title type="html">Three papers accepted to ICASSP.</title><link href="http://localhost:4000/news/2024/02/01/ICASSP.html" rel="alternate" type="text/html" title="Three papers accepted to ICASSP." /><published>2024-02-01T10:00:00-05:00</published><updated>2024-02-01T10:00:00-05:00</updated><id>http://localhost:4000/news/2024/02/01/ICASSP</id><content type="html" xml:base="http://localhost:4000/news/2024/02/01/ICASSP.html"><![CDATA[<p>Three <a href="/publications/">papers</a> were accepted to the upcoming <a href="https://2024.ieeeicassp.org">ICASSP 2024</a> conference.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Three papers were accepted to the upcoming ICASSP 2024 conference.]]></summary></entry><entry><title type="html">TorchCrepe pitch tracker: 2,000,0000+ downloads.</title><link href="http://localhost:4000/news/2023/10/18/TorchCrepe.html" rel="alternate" type="text/html" title="TorchCrepe pitch tracker: 2,000,0000+ downloads." /><published>2023-10-18T11:00:00-04:00</published><updated>2023-10-18T11:00:00-04:00</updated><id>http://localhost:4000/news/2023/10/18/TorchCrepe</id><content type="html" xml:base="http://localhost:4000/news/2023/10/18/TorchCrepe.html"><![CDATA[<p>Lab member Max Morrison’s <a href="https://github.com/maxrmorrison/torchcrepe">TorchCrepe</a> pitch tracker implementation passes 2,000,0000 downloads.
Click <a href="https://github.com/maxrmorrison/torchcrepe"><strong>HERE</strong></a> to try out TorchCrepe for youreslf!</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Lab member Max Morrison’s TorchCrepe pitch tracker implementation passes 2,000,0000 downloads. Click HERE to try out TorchCrepe for youreslf!]]></summary></entry><entry><title type="html">Try the VampNet demo: Music Generation via Masked Acoustic Token Modeling.</title><link href="http://localhost:4000/news/2023/07/01/VampNet.html" rel="alternate" type="text/html" title="Try the VampNet demo: Music Generation via Masked Acoustic Token Modeling." /><published>2023-07-01T11:00:00-04:00</published><updated>2023-07-01T11:00:00-04:00</updated><id>http://localhost:4000/news/2023/07/01/VampNet</id><content type="html" xml:base="http://localhost:4000/news/2023/07/01/VampNet.html"><![CDATA[<p>Our paper <a href="/project/music-audio-generation.html">“VAMPNET: Music Generation via Masked Acoustic Token Modeling”</a> was accepted to ISMIR 2023.</p>

<p>Click <a href="https://huggingface.co/spaces/descript/vampnet"><strong>HERE</strong></a> to try out VampNet!</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Our paper “VAMPNET: Music Generation via Masked Acoustic Token Modeling” was accepted to ISMIR 2023.]]></summary></entry><entry><title type="html">$440K grant from NSF: Engaging Blind and Visually Impaired Youth in Computer Science through Music Programming</title><link href="http://localhost:4000/news/2023/06/01/NSF-grant.html" rel="alternate" type="text/html" title="$440K grant from NSF: Engaging Blind and Visually Impaired Youth in Computer Science through Music Programming" /><published>2023-06-01T11:00:00-04:00</published><updated>2023-06-01T11:00:00-04:00</updated><id>http://localhost:4000/news/2023/06/01/NSF-grant</id><content type="html" xml:base="http://localhost:4000/news/2023/06/01/NSF-grant.html"><![CDATA[<p>Our collaboration with Georgia Tech, entitiled <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2300633&amp;HistoricalAwards=false">“Collaborative Research: Engaging Blind and Visually Impaired Youth in Computer Science through Music Programming”.</a> has been funded by the Discovery Research preK-12 (DRK-12) program, the Innovative Technology Experiences for Students and Teachers (ITEST) program, and the  the CS for All: Research and RPPs program.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Our collaboration with Georgia Tech, entitiled “Collaborative Research: Engaging Blind and Visually Impaired Youth in Computer Science through Music Programming”. has been funded by the Discovery Research preK-12 (DRK-12) program, the Innovative Technology Experiences for Students and Teachers (ITEST) program, and the the CS for All: Research and RPPs program.]]></summary></entry><entry><title type="html">Our tech inside Adobe’s new AI-powered audio editor</title><link href="http://localhost:4000/news/2023/06/01/adobe-podcast.html" rel="alternate" type="text/html" title="Our tech inside Adobe’s new AI-powered audio editor" /><published>2023-06-01T11:00:00-04:00</published><updated>2023-06-01T11:00:00-04:00</updated><id>http://localhost:4000/news/2023/06/01/adobe-podcast</id><content type="html" xml:base="http://localhost:4000/news/2023/06/01/adobe-podcast.html"><![CDATA[<p>Adobe just announced their <a href="https://podcast.adobe.com">Podcast</a> web-based AI-powered audio editing and recording tools. The Mic Check tool in Podcast applies work from our patent <a href="https://patents.google.com/patent/US11138989B2/en?inventor=bryan+pardo&amp;oq=bryan+pardo">“Sound quality prediction and interface to facilitate high-quality voice recordings”</a> to ensure users make better recordings.</p>

<p>See the <a href="https://www.youtube.com/watch?v=X4LV-9OPYJ0&amp;t=1s">Voice Assist video</a> or the our 2019 CHI paper <a href="https://interactiveaudiolab.github.io/assets/papers/seetharaman_voiceassist_chi19.pdf">“VoiceAssist: Guiding Users to High-Quality Voice Recordings”</a> for the technical details.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Adobe just announced their Podcast web-based AI-powered audio editing and recording tools. The Mic Check tool in Podcast applies work from our patent “Sound quality prediction and interface to facilitate high-quality voice recordings” to ensure users make better recordings.]]></summary></entry><entry><title type="html">Lexie B2 hearing aids use our tech</title><link href="http://localhost:4000/news/2022/12/01/bose-hearing-aids.html" rel="alternate" type="text/html" title="Lexie B2 hearing aids use our tech" /><published>2022-12-01T10:00:00-05:00</published><updated>2022-12-01T10:00:00-05:00</updated><id>http://localhost:4000/news/2022/12/01/bose-hearing-aids</id><content type="html" xml:base="http://localhost:4000/news/2022/12/01/bose-hearing-aids.html"><![CDATA[<p>The (https://lexiehearing.com/us/lexie-b2-powered-by-bose-hearing-aids}[Lexie B2 hearing aids] were rated the best over-the-counter hearing aids of 2022 by both Money Magazine and USA Today.  They use the work in our patent <a href="https://patents.google.com/patent/USRE48462E1/en?inventor=bryan+pardo&amp;oq=bryan+pardo">“Systems, methods, and apparatus for equalization preference learning”</a>.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[The (https://lexiehearing.com/us/lexie-b2-powered-by-bose-hearing-aids}[Lexie B2 hearing aids] were rated the best over-the-counter hearing aids of 2022 by both Money Magazine and USA Today. They use the work in our patent “Systems, methods, and apparatus for equalization preference learning”.]]></summary></entry><entry><title type="html">$100K grant from Sony to fund speech generation</title><link href="http://localhost:4000/news/2022/10/01/Sony-grant.html" rel="alternate" type="text/html" title="$100K grant from Sony to fund speech generation" /><published>2022-10-01T11:00:00-04:00</published><updated>2022-10-01T11:00:00-04:00</updated><id>http://localhost:4000/news/2022/10/01/Sony-grant</id><content type="html" xml:base="http://localhost:4000/news/2022/10/01/Sony-grant.html"><![CDATA[<p>Sony corporation has awarded us $100,000 as part of their Research Award Program to fund our research grant titled “Expressive and Controllable Voice Synthesis and Conversion”.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[Sony corporation has awarded us $100,000 as part of their Research Award Program to fund our research grant titled “Expressive and Controllable Voice Synthesis and Conversion”.]]></summary></entry><entry><title type="html">$1.8 million Future of Work award from NSF</title><link href="http://localhost:4000/news/2022/09/12/NSF-grant.html" rel="alternate" type="text/html" title="$1.8 million Future of Work award from NSF" /><published>2022-09-12T11:00:00-04:00</published><updated>2022-09-12T11:00:00-04:00</updated><id>http://localhost:4000/news/2022/09/12/NSF-grant</id><content type="html" xml:base="http://localhost:4000/news/2022/09/12/NSF-grant.html"><![CDATA[<p>TEAMuP, Our collaboration with the University of Rochester has been awarded a $1.8 million Future of Work at the Human-Technology Frontier award from NSF. For more details on what we’re doing see our project <a href="/project/audacity.html">Deep Learning Tools for Audacity</a> or see <a href="https://www.mccormick.northwestern.edu/computer-science/news-events/news/articles/2022/connecting-deep-learning-developers-with-sound-artists.html">this story</a>.</p>]]></content><author><name></name></author><category term="news" /><summary type="html"><![CDATA[TEAMuP, Our collaboration with the University of Rochester has been awarded a $1.8 million Future of Work at the Human-Technology Frontier award from NSF. For more details on what we’re doing see our project Deep Learning Tools for Audacity or see this story.]]></summary></entry></feed>