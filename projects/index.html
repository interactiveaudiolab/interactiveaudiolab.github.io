<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Projects | Interactive Audio Lab</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Projects" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Lab description" />
<meta property="og:description" content="Lab description" />
<link rel="canonical" href="http://localhost:4000/projects/" />
<meta property="og:url" content="http://localhost:4000/projects/" />
<meta property="og:site_name" content="Interactive Audio Lab" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Projects" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Lab description","headline":"Projects","url":"http://localhost:4000/projects/"}</script>
<!-- End Jekyll SEO tag -->
<link href="https://fonts.googleapis.com/css?family=Oswald|Roboto+Condensed" rel="stylesheet">
  <link rel="stylesheet" href="/assets/css/main.css""><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Interactive Audio Lab" /></head>
<body>
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a><header>
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark primary-nav" aria-label="primary navigation">
    <div class="container">
      <a class="navbar-brand display" href="/">INTERACTIVE AUDIO LAB</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
    
      <!-- main navigation -->
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto"> 
            <li class="nav-item">
              
                <a class="nav-link active" href="/projects/">
                  Projects
                </a>
              
            </li>
           
            <li class="nav-item">
              
                <a class="nav-link " href="/publications/">
                  Publications
                </a>
              
            </li>
           
            <li class="nav-item">
              
                <a class="nav-link " href="/resources/">
                  Resources
                </a>
              
            </li>
           
            <li class="nav-item">
              
                <a class="nav-link " href="/people/">
                  People
                </a>
              
            </li>
          
    
        </ul>

        <!-- Social media links -->
        <ul class="navbar-nav mr-sm-2">
            <li class="nav-item">
                <a class="nav-link external-link social-link" href="https://github.com/interactiveaudiolab" target="_blank"><span class="fab fa-github" title="Interactive Audio Lab on GitHub"></span><span class="sr-only">Link opens in a new window</span></a>
            </li>
            <li class="nav-item">
                <a class="nav-link external-link social-link" href="https://www.youtube.com/channel/UCY-jggB_-R3rYaTsWN2_Ssw" target="_blank"><span class="fab fa-youtube" title="Interactive Audio Lab on YouTube"></span><span class="sr-only">Link opens in a new window</span></a>
              </li>
        </ul>
      </div>
    </div> <!-- end container -->
  </nav>

  <nav class="navbar navbar-expand-lg navbar-dark bg-primary subnav" aria-label="subnavigation">
  </nav>
</header>

<div class="container">
    <main id="content" class="content" tabindex=-1 aria-label="Content">
        <header>
        <h1 class="display-5">Projects</h1>
</header>

<hr />

<nav class="sub-nav">
    <ul class="nav">
    
    <li class="nav-item">
        <a class="nav-link" href="#generation">generation</a>
    </li>
    
    <li class="nav-item">
        <a class="nav-link" href="#interfaces">interfaces</a>
    </li>
    
    <li class="nav-item">
        <a class="nav-link" href="#robustness">robustness</a>
    </li>
    
    <li class="nav-item">
        <a class="nav-link" href="#separation">separation</a>
    </li>
    
    <li class="nav-item">
        <a class="nav-link" href="#search">search</a>
    </li>
    
    </ul>
</nav>

<h2 id="generation">Generation</h2>
<p>Audio generation leverages generative machine learning models (e.g., Variational Autoencoders or Generative Adversarial Networks) to create an audio waveform or a symbolic representation of audio (e.g., MIDI). This includes tasks such as music generation and text-to-speech (TTS). These generative models can be unconditioned (e.g., generating any kind of music without user input) or conditioned (e.g., generating jazz-rock played on a cello where the first eight bars are the same as Beethoven’s Fifth Symphony). Conditional audio generation has the potential to enable novel tools for composers, dialogue editors for film and podcasts, and sound designers. For further publications in this area, see our <a href="/publications">publications</a> page.</p>

<!-- collection, title, and anchor-tag needs to be passed into this to work -->

<div>

    <h2 class="display-6 people-header"><a name="generation"></a></h2>

    <ul class="list-unstyled preview-list">
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/sketch2sound.png" alt="System description" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/sketch2sound.html">Sketch2Sound - Controllable Audio Generation via Time-Varying Signals and Sonic Imitations</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>In collaboration with Adobe, we present Sketch2Sound, a generative audio model capable of creating high-quality sounds from a set of interpretable time-varying control signals: loudness, brightness, and pitch, as well as text prompts. Sketch2Sound can synthesize arbitrary sounds from sonic imitations (i.e., a vocal imitation or a reference sound-shape). Sketch2Sound can be implemented on top of any text-to-audio latent diffusion transformer (DiT), and requires only 40k steps of fine-tuning and a single linear layer per control, making it more lightweight than existing methods like ControlNet.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/tria.jpg" alt="System description" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/tria.html">The Rhythm in Anything</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>we present TRIA (The Rhythm In Anything), a masked transformer model for mapping rhythmic sound gestures to high-fidelity drum recordings. Given an audio prompt of the desired rhythmic pattern and a second prompt to represent drumkit timbre, TRIA produces audio of a drumkit playing the desired rhythm (with appropriate elaborations)in the desired timbre.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/text2fx.png" alt="System description" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/text2fx.html">Text2FX - Harnessing CLAP Embeddings for Text-Guided Audio Effects</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>Text2FX leverages CLAP embeddings and differentiable digital signal processing to control audio effects, such as equalization and reverberation, using open-vocabulary natural language prompts (e.g., “make this sound in-your-face and bold”).</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/speech.png" alt="Speech conversation icon" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/controllable-speech-generation.html">Controllable Speech Generation</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>Nuances in speech prosody (i.e., the pitch, timing, and loudness of speech) are a vital part of how we communicate. We develop generative machine learning models that use interpretable, disentangled representations of speech to give control over these nuances and generate speech reflecting user-specified prosody.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/maskmark_specplot.png" alt="MaskMark" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/maskmark.html">MaskMark - Robust Neural Watermarking for Real and Synthetic Speech</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>High-quality speech synthesis models may be used to spread misinformation or impersonate voices. Audio watermarking can combat misuse by embedding a traceable signature in generated audio. However, existing audio watermarks typically demonstrate robustness to only a small set of transformations of the watermarked audio. To address this, we propose MaskMark, a neural network-based digital audio watermarking technique optimized for speech.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/vampnet.png" alt="System description" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/music-audio-generation.html">VampNet - Music Generation via Masked Acoustic Token Modeling</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We introduce VampNet, a masked acoustic token modeling approach to music audio generation. VampNet lets us sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference.  Prompting VampNet appropriately, enables music compression, inpainting, outpainting, continuation, and looping with variation (vamping). This makes VampNet a powerful music co-creation tool.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/treble.png" alt="Treble clef" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/symbolic-music-generation.html">Symbolic music generation</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>Symbolic music generation uses machine learning to produce music in a symbolic form, such as the Musical Instrument Digital Interface (MIDI) format. Generating music in a symbolic format has the advantages of being both interpretable (e.g., as pitch, duration, and loudness values) and editable in standard digital audio workstations (DAWs).</p>

</p>
                </div>
            </li>
        
    </ul>
</div>

<p><br /></p>

<h2 id="interfaces">Interfaces</h2>
<p>Improving audio production tools meaningfully enhances the creative output of musicians, podcasters, producers and videographers. We focus on bridging the gap between the intentions of creators and the interfaces of audio recording and manipulation tools they use. Our work in this area has a strong human-centered machine learning component.  Representative projects in the area are below. For further publications in this area, see our <a href="/publications">publications</a> page.</p>

<!-- collection, title, and anchor-tag needs to be passed into this to work -->

<div>

    <h2 class="display-6 people-header"><a name="interfaces"></a></h2>

    <ul class="list-unstyled preview-list">
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/audacity-logo.png" alt="Audacity logo" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/audacity.html">Deep Learning Tools for Audacity</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We provide a software framework that lets deep learning practitioners easily integrate their own PyTorch models into the open-source Audacity DAW. This lets ML audio researchers put tools in the hands of sound artists without doing DAW-specific development work.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/hands-over-eyes-150by150-shutterstock_354081641.png" alt="Man with hands over his eyes" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/eyes-free-interfaces.html">Eyes Free Audio Production</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>This project focuses on building novel accessible tools for creating audio-based content like music or podcasts. The tools should support the needs of blind creators, whether working independently or on teams with sighted collaborators.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/harp-logo.png" alt="a cartoon harp" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/harp.html">HARP - Bringing Deep Learning to the DAW with Hosted, Asynchronous, Remote Processing</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p><a href="https://github.com/audacitorch/HARP/tree/main">HARP</a> is an ARA plug-in that allows for hosted, asynchronous, remote processing of audio with deep learning models. HARP works by routing audio from a digital audio workstation (DAW) through Gradio endpoints. Because Gradio apps can be hosted locally or in the cloud (e.g., HuggingFace Spaces), HARP lets users of Digital Audio Workstations (e.g. Reaper) access large state-of-the-art models in the cloud, without breaking their within-DAW workflow.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/synthassist-150x150.png" alt="Picture of the SynthAssist user interface" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/interfaces-that-learn.html">Audio production interfaces that learn from user interaction</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We use metaphors and techniques familiar to musicians to produce customizable environments for music creation, with a focus on bridging the gap between the intentions of both amateur and professional musicians and the audio manipulation tools available through software.</p>

</p>
                </div>
            </li>
        
    </ul>
</div>

<p><br /></p>

<h2 id="robustness">Robustness</h2>
<p>Neural network-based audio interfaces should be robust to various input distortions, especially in sensitive applications. We study the behavior of audio models under maliciously-crafted inputs - called <em>adversarial examples</em> - in order to better understand how to secure audio interfaces against bad-faith actors and naturally-occurring distortions. For further publications in this area, see our <a href="/publications">publications</a> page.</p>

<!-- collection, title, and anchor-tag needs to be passed into this to work -->

<div>

    <h2 class="display-6 people-header"><a name="robustness"></a></h2>

    <ul class="list-unstyled preview-list">
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/filters.png" alt="Adaptive filtering" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/audio-adversarial-examples.html">Audio adversarial examples with adaptive filtering</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We demonstrate a novel audio-domain adversarial attack that modifies benign audio using an interpretable and differentiable parametric transformation - adaptive filtering. Unlike existing state-of-the-art attacks, our proposed method does not require a complex optimization procedure or generative model, relying only on a simple variant of gradient descent to tune filter parameters.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/voiceblock_arch.png" alt="VoiceBlock" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/voiceblock.html">Privacy through Real-Time Adversarial Attacks with Audio-to-Audio Models</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>As governments and corporations adopt deep learning systems to apply voice ID at scale, concerns about security and privacy naturally emerge. We propose a neural network model capable of inperceptibly modifying a user’s voice in real-time to prevent speaker recognition from identifying their voce.</p>

</p>
                </div>
            </li>
        
    </ul>
</div>

<p><br /></p>

<h2 id="separation">Separation</h2>
<p>Audio source separation is the process of extracting a single sound (e.g. one violin) from a mixture of sounds (a string quartet). This is an ongoing research area in the lab. Source separation is the audio analog of scene segmentation in computer vision and is a foundational technology that improves or enables speech recogntion, sound object labeling, music transcription,hearing aids and other technologies. For further publications in this area, see our <a href="/publications">publications</a> page.</p>

<!-- collection, title, and anchor-tag needs to be passed into this to work -->

<div>

    <h2 class="display-6 people-header"><a name="separation"></a></h2>

    <ul class="list-unstyled preview-list">
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/MSG-thumbnail.png" alt="spectrogram display of improvement from MSG" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/msg.html">Music Separation Enhancement with Generative Modeling</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We introduce Make it Sound Good (MSG), a post-processor that enhances the output quality of source separation systems like Demucs, Wavenet, Spleeter, and OpenUnmix.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/Love_in_the_Goat_World-150x150-wikimedia-cc-by-sa-3p0.jpg" alt="Picture of two goats nussling" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/nussl.html">nussl</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>The <a href="https://github.com/interactiveaudiolab/nussl">Northwestern University Source Separation Library (nussl)</a> provides implementations of common audio source separation algorithms as well as an easy-to-use framework for prototyping and adding new algorithms.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/bootstrapping.png" alt="System diagram of bootstrapping computer audition" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/bootstrapping.html">Bootstrapping deep learning models for computer audition</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We are developing methods that allow a deep learning model learn to segment and label an audio scene (e.g. separate and label all the overlapped talkers at a cocktail party) without ever having been taught from isolated sound sources.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/cerberus.png" alt="System diagram of Cerberus" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/cerberus.html">Cerberus, simultaneous audio separation and transcription</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>Cerberus is a single deep learning architecture that can simultaneously separate sources in a musical mixture and transcribe those sources.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/mcft.png" alt="MCFT filter" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/mcft.html">Multi-resolution Common Fate Transform</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>The <a href="/assets/papers/pishdadian_pardo_mcft_journal_2018.pdf">Multi-resolution Common Fate Transform (MCFT)</a> is an audio signal representation developed in our lab in the context of audio source separation.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/tagbox.png" alt="System diagram of TagBox" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/tagbox.html">Unsupervised Source Separation By Steering Pretrained Music Models</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We showcase an unsupervised method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining.</p>

</p>
                </div>
            </li>
        
    </ul>
</div>

<p><br /></p>

<h2 id="search">Search</h2>
<p>Content-addressable search through collections of many audio files (thousands) or lengthy audio files (hours) is an ongoing research area. In this work, we develop and apply cutting edge techniques in machine learning, signal processing and interface design. This is part of a <a href="http://www2.ece.rochester.edu/projects/air/projects/audiosearch">collaboration with the University of Rochester AIR lab</a> and is supported by the National Science Foundation. Representative recent projects in this area are below. For further publications in this area, see our <a href="/publications">publications</a> page.</p>

<!-- collection, title, and anchor-tag needs to be passed into this to work -->

<div>

    <h2 class="display-6 people-header"><a name="search"></a></h2>

    <ul class="list-unstyled preview-list">
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/ised.png" alt="ISED logo" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/ised.html">ISED</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>Interactive Sound Event Detector (I-SED) is a human-in-the-loop interface for sound event annotation that helps users label sound events of interest within a lengthy recording quickly. The annotation is performed by a collaboration between a user and a machine.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/hierarchical-protonets.png" alt="Hierarchical Prototypical Networks" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/music-trees.html">Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>In this work, we exploit hierarchical relationships between instruments in a few-shot learning setup to enable classification of a wider set of musical instruments, given a few examples at inference.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/voogle.png" alt="Voogle logo" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/voogle.html">Voogle</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p><a href="https://github.com/interactiveaudiolab/voogle">Voogle</a> is an audio search engine that lets users search a database of sounds by vocally imitating or providing an example of the sound they are searching for.</p>

</p>
                </div>
            </li>
        
    </ul>
</div>

<p><br /></p>


    </main>
  </div>
  <br></br><footer class="footer bg-light" role="contentinfo">

    <div class="container">

      <div class="row align-items-center footer-copy">
            &copy; 2025 Interactive Audio Lab
            <a class="external-link social-link" href="https://github.com/interactiveaudiolab" target="_blank"><span class="fab fa-github" title="Interactive Audio Lab on GitHub"></span><span class="sr-only">Link opens in a new window</span></a>
            <a class="external-link social-link" href="https://www.youtube.com/channel/UCY-jggB_-R3rYaTsWN2_Ssw" target="_blank"><span class="fab fa-youtube" title="Interactive Audio Lab on YouTube"></span><span class="sr-only">Link opens in a new window</span></a>
    </div>

</footer>

<!-- js scripts for bootstrap and fontawesome -->
<script defer src="https://use.fontawesome.com/releases/v5.0.8/js/all.js" integrity="sha384-SlE991lGASHoBfWbelyBPLsUlwY1GwNDJo3jSJO04KZ33K2bwfV9YBauFfnzvynJ" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script></body>

</html>
