<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Projects | Interactive Audio Lab</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Projects" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Lab description" />
<meta property="og:description" content="Lab description" />
<link rel="canonical" href="http://localhost:4000/projects/" />
<meta property="og:url" content="http://localhost:4000/projects/" />
<meta property="og:site_name" content="Interactive Audio Lab" />
<script type="application/ld+json">
{"description":"Lab description","url":"http://localhost:4000/projects/","@type":"WebPage","headline":"Projects","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link href="https://fonts.googleapis.com/css?family=Oswald|Roboto+Condensed" rel="stylesheet">
  <link rel="stylesheet" href="/assets/css/main.css""><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Interactive Audio Lab" /></head>
<body>
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a><header>
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark primary-nav" aria-label="primary navigation">
    <div class="container">
      <a class="navbar-brand display" href="/">INTERACTIVE AUDIO LAB</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
    
      <!-- main navigation -->
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto"> 
            <li class="nav-item">
              
                <a class="nav-link active" href="/projects/">
                  Projects
                </a>
              
            </li>
           
            <li class="nav-item">
              
                <a class="nav-link " href="/publications/">
                  Publications
                </a>
              
            </li>
           
            <li class="nav-item">
              
                <a class="nav-link " href="/resources/">
                  Resources
                </a>
              
            </li>
           
            <li class="nav-item">
              
                <a class="nav-link " href="/people/">
                  People
                </a>
              
            </li>
          
    
        </ul>

        <!-- Social media links -->
        <ul class="navbar-nav mr-sm-2">
            <li class="nav-item">
                <a class="nav-link external-link social-link" href="https://github.com/interactiveaudiolab" target="_blank"><span class="fab fa-github" title="Interactive Audio Lab on GitHub"></span><span class="sr-only">Link opens in a new window</span></a>
            </li>
            <li class="nav-item">
                <a class="nav-link external-link social-link" href="https://www.youtube.com/channel/UCY-jggB_-R3rYaTsWN2_Ssw" target="_blank"><span class="fab fa-youtube" title="Interactive Audio Lab on YouTube"></span><span class="sr-only">Link opens in a new window</span></a>
              </li>
        </ul>
      </div>
    </div> <!-- end container -->
  </nav>

  <nav class="navbar navbar-expand-lg navbar-dark bg-primary subnav" aria-label="subnavigation">
  </nav>
</header>

<div class="container">
    <main id="content" class="content" tabindex=-1 aria-label="Content">
        <header>
        <h1 class="display-5">Projects</h1>
</header>

<hr />

<nav class="sub-nav">
    <ul class="nav">
    
    <li class="nav-item">
        <a class="nav-link" href="#interfaces">interfaces</a>
    </li>
    
    <li class="nav-item">
        <a class="nav-link" href="#search">search</a>
    </li>
    
    <li class="nav-item">
        <a class="nav-link" href="#separation">separation</a>
    </li>
    
    <li class="nav-item">
        <a class="nav-link" href="#generation">generation</a>
    </li>
    
    <li class="nav-item">
        <a class="nav-link" href="#robustness">robustness</a>
    </li>
    
    </ul>
</nav>

<h2 id="interfaces">Interfaces</h2>
<p>Improving audio production tools meaningfully enhances the creative output of musicians, podcasters, producers and videographers. We focus on bridging the gap between the intentions of creators and the interfaces of audio recording and manipulation tools they use. Our work in this area has a strong human-centered machine learning component.  Representative projects in the area are below. For further publications in this area, see our <a href="/publications">publications</a> page.</p>

<!-- collection, title, and anchor-tag needs to be passed into this to work -->

<div>

    <h2 class="display-6 people-header"><a name="interfaces"></a></h2>

    <ul class="list-unstyled preview-list">
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/audacity-logo.png" alt="Audacity logo" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/audacity.html">Deep Learning Tools for Audacity</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We provide a software framework that lets deep learning practitioners easily integrate their own PyTorch models into the open-source Audacity DAW. This lets ML audio researchers put tools in the hands of sound artists without doing DAW-specific development work.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/hands-over-eyes-150by150-shutterstock_354081641.png" alt="Man with hands over his eyes" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/eyes-free-interfaces.html">Eyes Free Audio Production</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>This project focuses on building novel accessible tools for creating audio-based content like music or podcasts. The tools should support the needs of blind creators, whether working independently or on teams with sighted collaborators.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/synthassist-150x150.png" alt="Picture of the SynthAssist user interface" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/interfaces-that-learn.html">Audio production interfaces that learn from user interaction</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We use metaphors and techniques familiar to musicians to produce customizable environments for music creation, with a focus on bridging the gap between the intentions of both amateur and professional musicians and the audio manipulation tools available through software.</p>

</p>
                </div>
            </li>
        
    </ul>
</div>

<h2 id="search">Search</h2>
<p>Content-addressable search through collections of many audio files (thousands) or lengthy audio files (hours) is an ongoing research area. In this work, we develop and apply cutting edge techniques in machine learning, signal processing and interface design. This is part of a <a href="http://www2.ece.rochester.edu/projects/air/projects/audiosearch">collaboration with the University of Rochester AIR lab</a> and is supported by the National Science Foundation. Representative recent projects in this area are below. For further publications in this area, see our <a href="/publications">publications</a> page.</p>

<!-- collection, title, and anchor-tag needs to be passed into this to work -->

<div>

    <h2 class="display-6 people-header"><a name="search"></a></h2>

    <ul class="list-unstyled preview-list">
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/ised.png" alt="ISED logo" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/ised.html">ISED</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>Interactive Sound Event Detector (I-SED) is a human-in-the-loop interface for sound event annotation that helps users label sound events of interest within a lengthy recording quickly. The annotation is performed by a collaboration between a user and a machine.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/hierarchical-protonets.png" alt="Hierarchical Prototypical Networks" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/music-trees.html">Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>In this work, we exploit hierarchical relationships between instruments in a few-shot learning setup to enable classification of a wider set of musical instruments, given a few examples at inference.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/voogle.png" alt="Voogle logo" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/voogle.html">Voogle</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p><a href="https://github.com/interactiveaudiolab/voogle">Voogle</a> is an audio search engine that lets users search a database of sounds by vocally imitating or providing an example of the sound they are searching for.</p>

</p>
                </div>
            </li>
        
    </ul>
</div>

<p><br /></p>

<h2 id="separation">Separation</h2>
<p>Audio source separation is the process of extracting a single sound (e.g. one violin) from a mixture of sounds (a string quartet). This is an ongoing research area in the lab. Source separation is the audio analog of scene segmentation in computer vision and is a foundational technology that improves or enables speech recogntion, sound object labeling, music transcription,hearing aids and other technologies. For further publications in this area, see our <a href="/publications">publications</a> page.</p>

<!-- collection, title, and anchor-tag needs to be passed into this to work -->

<div>

    <h2 class="display-6 people-header"><a name="separation"></a></h2>

    <ul class="list-unstyled preview-list">
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/MSG-thumbnail.png" alt="spectrogram display of improvement from MSG" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/msg.html">Music Separation Enhancement with Generative Modeling</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We introduce Make it Sound Good (MSG), a post-processor that enhances the output quality of source separation systems like Demucs, Wavenet, Spleeter, and OpenUnmix.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/Love_in_the_Goat_World-150x150-wikimedia-cc-by-sa-3p0.jpg" alt="Picture of two goats nussling" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/nussl.html">nussl</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>The <a href="https://github.com/interactiveaudiolab/nussl">Northwestern University Source Separation Library (nussl)</a> provides implementations of common audio source separation algorithms as well as an easy-to-use framework for prototyping and adding new algorithms.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/bootstrapping.png" alt="System diagram of bootstrapping computer audition" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/bootstrapping.html">Bootstrapping deep learning models for computer audition</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We are developing methods that allow a deep learning model learn to segment and label an audio scene (e.g. separate and label all the overlapped talkers at a cocktail party) without ever having been taught from isolated sound sources.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/cerberus.png" alt="System diagram of Cerberus" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/cerberus.html">Cerberus, simultaneous audio separation and transcription</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>Cerberus is a single deep learning architecture that can simultaneously separate sources in a musical mixture and transcribe those sources.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/mcft.png" alt="MCFT filter" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/mcft.html">Multi-resolution Common Fate Transform</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>The <a href="/assets/papers/pishdadian_pardo_mcft_journal_2018.pdf">Multi-resolution Common Fate Transform (MCFT)</a> is an audio signal representation developed in our lab in the context of audio source separation.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/tagbox.png" alt="System diagram of TagBox" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/tagbox.html">Unsupervised Source Separation By Steering Pretrained Music Models</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We showcase an unsupervised method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining.</p>

</p>
                </div>
            </li>
        
    </ul>
</div>

<p><br /></p>

<h2 id="generation">Generation</h2>
<p>Audio generation leverages generative machine learning models (e.g., Variational Autoencoders or Generative Adversarial Networks) to create an audio waveform or a symbolic representation of audio (e.g., MIDI). This includes tasks such as music generation and text-to-speech (TTS). These generative models can be unconditioned (e.g., generating any kind of music without user input) or conditioned (e.g., generating jazz-rock played on a cello where the first eight bars are the same as Beethoven’s Fifth Symphony). Conditional audio generation has the potential to enable novel tools for composers, dialogue editors for film and podcasts, and sound designers. For further publications in this area, see our <a href="/publications">publications</a> page.</p>

<!-- collection, title, and anchor-tag needs to be passed into this to work -->

<div>

    <h2 class="display-6 people-header"><a name="generation"></a></h2>

    <ul class="list-unstyled preview-list">
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/speech.png" alt="Speech conversation icon" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/controllable-speech-generation.html">Controllable Speech Generation</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>Nuances in speech prosody (i.e., the pitch, timing, and loudness of speech) are a vital part of how we communicate. We utilize generative machine learning models to generate prosody with user control over these nuances and generate speech reflecting user-specified prosody.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/treble.png" alt="Treble clef" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/symbolic-music-generation.html">Symbolic music generation</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>Symbolic music generation uses machine learning to produce music in a symbolic form, such as the Musical Instrument Digital Interface (MIDI) format. Generating music in a symbolic format has the advantages of being both interpretable (e.g., as pitch, duration, and loudness values) and editable in standard digital audio workstations (DAWs).</p>

</p>
                </div>
            </li>
        
    </ul>
</div>

<p><br /></p>

<h2 id="robustness">Robustness</h2>
<p>Neural network-based audio interfaces should be robust to various input distortions, especially in sensitive applications. We study the behavior of audio models under maliciously-crafted inputs - called <em>adversarial examples</em> - in order to better understand how to secure audio interfaces against bad-faith actors and naturally-occurring distortions. For further publications in this area, see our <a href="/publications">publications</a> page.</p>

<!-- collection, title, and anchor-tag needs to be passed into this to work -->

<div>

    <h2 class="display-6 people-header"><a name="robustness"></a></h2>

    <ul class="list-unstyled preview-list">
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/filters.png" alt="Adaptive filtering" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/audio-adversarial-examples.html">Audio adversarial examples with adaptive filtering</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>We demonstrate a novel audio-domain adversarial attack that modifies benign audio using an interpretable and differentiable parametric transformation - adaptive filtering. Unlike existing state-of-the-art attacks, our proposed method does not require a complex optimization procedure or generative model, relying only on a simple variant of gradient descent to tune filter parameters.</p>

</p>
                </div>
            </li>
        
            <li class="media my-2">
                
                <div>
                    <img class="mr-3" src="/assets/images/projects/voiceblock_arch.png" alt="VoiceBlock" />
                </div>
                

                <div class="mt-0 mb-1">
                    <h3 class="mt-0"><a href="/project/voiceblock.html">Privacy through Real-Time Adversarial Attacks with Audio-to-Audio Models</a></h3>
                    <h4 class=" h5 text-muted"></h4>
                    <p><p>As governments and corporations adopt deep learning systems to collect and analyze user-generated audio data, concerns about security and privacy naturally emerge in areas such as automatic speaker recognition. While audio adversarial examples offer one route to mislead or evade these invasive systems, they are typically crafted through time-intensive offline optimization, limiting their usefulness in streaming contexts. Inspired by architectures for audio-to-audio tasks such as denoising and speech enhancement, we propose a neural network model capable of adversarially modifying a user’s audio stream in real-time. Our model learns to apply a time-varying finite impulse response (FIR) filter to outgoing audio, allowing for effective and inconspicuous perturbations on a small fixed delay suitable for streaming tasks. We demonstrate our model is highly effective at de-identifying user speech from speaker recognition and able to transfer to an unseen recognition system. We conduct a perceptual study and find that our method produces perturbations significantly less perceptible than baseline anonymization methods, when controlling for effectiveness. Finally, we provide an implementation of our model capable of running in real-time on a single CPU thread.</p>

</p>
                </div>
            </li>
        
    </ul>
</div>

<p><br /></p>


    </main>
  </div>
  <br></br><footer class="footer bg-light" role="contentinfo">

    <div class="container">

      <div class="row align-items-center footer-copy">
            &copy; 2022 Interactive Audio Lab
            <a class="external-link social-link" href="https://github.com/interactiveaudiolab" target="_blank"><span class="fab fa-github" title="Interactive Audio Lab on GitHub"></span><span class="sr-only">Link opens in a new window</span></a>
            <a class="external-link social-link" href="https://www.youtube.com/channel/UCY-jggB_-R3rYaTsWN2_Ssw" target="_blank"><span class="fab fa-youtube" title="Interactive Audio Lab on YouTube"></span><span class="sr-only">Link opens in a new window</span></a>
    </div>

</footer>

<!-- js scripts for bootstrap and fontawesome -->
<script defer src="https://use.fontawesome.com/releases/v5.0.8/js/all.js" integrity="sha384-SlE991lGASHoBfWbelyBPLsUlwY1GwNDJo3jSJO04KZ33K2bwfV9YBauFfnzvynJ" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script></body>

</html>
